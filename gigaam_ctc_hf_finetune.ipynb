{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alt-f13/Dell_QA/blob/main/gigaam_ctc_hf_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libs\n",
        "%%capture\n",
        "!pip install -q evaluate\n",
        "!pip install -q jiwer\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -U transformers==4.49.0 accelerate==1.5.2 datasets==3.4.1\n"
      ],
      "metadata": {
        "id": "RUbOcN5YWkB-"
      },
      "id": "RUbOcN5YWkB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6",
      "metadata": {
        "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6"
      },
      "source": [
        "# Finetune GigaAM-v2-CTC with ðŸ¤— HuggingFace transformers\n",
        "\n",
        "GigaAM-v2 is an open-source models for Russian speech recognition tasks. SoTA in Russian ASR by March 2025.\n",
        "\n",
        "Original git: https://github.com/salute-developers/GigaAM\n",
        "\n",
        "This notebook is for finetuning CTC version of GigaAM-v2 model with `transformers` library.\n",
        "\n",
        "Use GPU environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d89fda",
      "metadata": {
        "id": "a5d89fda"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \".\"\n",
        "os.environ[\"WANDB_PROJECT\"] = \"project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fc5479",
      "metadata": {
        "id": "43fc5479"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "import datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import peft\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import Dataset, load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (AutoFeatureExtractor, AutoModel, AutoProcessor,\n",
        "                          AutoTokenizer, Trainer, TrainingArguments)\n",
        "from transformers.utils import is_datasets_available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f71c4805",
      "metadata": {
        "id": "f71c4805"
      },
      "outputs": [],
      "source": [
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33853aa8",
      "metadata": {
        "id": "33853aa8"
      },
      "source": [
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc985854",
      "metadata": {
        "id": "fc985854"
      },
      "outputs": [],
      "source": [
        "model_name = \"waveletdeboshir/gigaam-ctc\"\n",
        "SEED = 1234\n",
        "\n",
        "# Set max duration of audio files to 30 seconds\n",
        "MAX_DURATION = 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7ef5d3",
      "metadata": {
        "id": "1e7ef5d3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(SEED)\n",
        "pl.seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model, feature extractor and tokenizer\n",
        "\n",
        "These are `transformers` wrappers for GigaAM-CTC model, tokenizer and featuren extractor from https://huggingface.co/waveletdeboshir/gigaam-ctc"
      ],
      "metadata": {
        "id": "c304XZcs51oq"
      },
      "id": "c304XZcs51oq"
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "I_l7hkec51Tw"
      },
      "id": "I_l7hkec51Tw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def msize(m):\n",
        "    return sum(p.numel() for p in m.parameters())\n",
        "\n",
        "print(f\"N of parameters: {msize(model.model)}\")"
      ],
      "metadata": {
        "id": "K9hOjZv_6Nqm"
      },
      "id": "K9hOjZv_6Nqm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0",
      "metadata": {
        "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
      },
      "source": [
        "## Load Datasets\n",
        "\n",
        "We will load part of Golos dataset just for example. **GigaAM was already trained on this dataset. So use some other data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6215ea5d",
      "metadata": {
        "id": "6215ea5d"
      },
      "outputs": [],
      "source": [
        "audio_dataset = load_dataset(\"bond005/sberdevices_golos_10h_crowd\")\n",
        "audio_dataset[\"train\"] = audio_dataset[\"train\"].shuffle()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569f00ab",
      "metadata": {
        "id": "569f00ab"
      },
      "outputs": [],
      "source": [
        "audio_dataset[\"train\"] = audio_dataset[\"train\"].add_column(\"duration\", np.array([len(x[\"array\"]) / x[\"sampling_rate\"] for x in audio_dataset[\"train\"][\"audio\"]]))\n",
        "audio_dataset[\"validation\"] = audio_dataset[\"validation\"].add_column(\"duration\", np.array([len(x[\"array\"]) / x[\"sampling_rate\"] for x in audio_dataset[\"validation\"][\"audio\"]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8694c34f",
      "metadata": {
        "id": "8694c34f"
      },
      "outputs": [],
      "source": [
        "audio_dataset[\"train\"] = audio_dataset[\"train\"].filter(lambda x: x[\"duration\"] < MAX_DURATION)\n",
        "audio_dataset[\"validation\"] = audio_dataset[\"validation\"].filter(lambda x: x[\"duration\"] < MAX_DURATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f578c4",
      "metadata": {
        "id": "49f578c4"
      },
      "outputs": [],
      "source": [
        "audio_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
      "metadata": {
        "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6525c478-8962-4394-a1c4-103c54cce170",
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch, feature_extractor, tokenizer, text_column=\"text\", val=False):\n",
        "    \"\"\"\n",
        "    Compute log-Mel features.\n",
        "    Text tokenization.\"\"\"\n",
        "    # load and resample audio data\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    # if val:\n",
        "    feats = feature_extractor(\n",
        "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=\"longest\"\n",
        "    )\n",
        "    batch[\"input_features\"] = feats.input_features[0]\n",
        "    batch[\"input_lengths\"] = feats.input_lengths[0]\n",
        "    # else:\n",
        "    #     batch[\"input_features\"] = audio[\"array\"].copy()\n",
        "\n",
        "    batch[\"labels\"] = tokenizer(batch[text_column]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
      "metadata": {
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
      },
      "outputs": [],
      "source": [
        "audio_dataset[\"train\"] = audio_dataset[\"train\"].map(\n",
        "    partial(\n",
        "        prepare_dataset,\n",
        "        feature_extractor=feature_extractor,\n",
        "        tokenizer=tokenizer,\n",
        "        text_column=\"transcription\",\n",
        "        val=False,\n",
        "    ),\n",
        "    remove_columns=audio_dataset.column_names[\"train\"],\n",
        "    num_proc=1,\n",
        ")\n",
        "\n",
        "audio_dataset[\"validation\"] = audio_dataset[\"validation\"].map(partial(\n",
        "            prepare_dataset,\n",
        "            feature_extractor=feature_extractor,\n",
        "            tokenizer=tokenizer,\n",
        "            text_column=\"transcription\",\n",
        "            val=True\n",
        "            ), remove_columns=audio_dataset.column_names[\"validation\"],\n",
        "            num_proc=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241f8564",
      "metadata": {
        "id": "241f8564"
      },
      "outputs": [],
      "source": [
        "audio_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
      "metadata": {
        "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
      },
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
      "metadata": {
        "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
      },
      "source": [
        "### Define a Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    processor: Any\n",
        "    padding: str = \"longest\"\n",
        "    max_length: Optional[int] = 3001\n",
        "    max_length_tokens: Optional[int] = 1000\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": np.asarray(feature[\"input_features\"]).T} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, padding=self.padding, max_length=self.max_length, return_tensors=\"pt\")\n",
        "        batch[\"input_features\"] = batch[\"input_features\"].transpose(1, 2)\n",
        "\n",
        "        input_lengths = [feature[\"input_lengths\"] for feature in features]\n",
        "        # batch = self.processor.feature_extractor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, padding=self.padding, max_length=self.max_length_tokens, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"input_lengths\"] = torch.LongTensor(input_lengths)\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        if \"attention_mask\" in batch:\n",
        "            batch[\"attention_mask\"] = batch[\"attention_mask\"].to(torch.long)\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
      "metadata": {
        "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
      },
      "source": [
        "I want to use different collators for train and validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
      "metadata": {
        "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")\n",
        "val_data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"max_length\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
      "metadata": {
        "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22b4011-f31f-4b57-b684-c52332f92890",
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52",
      "metadata": {
        "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids)\n",
        "    label_str = tokenizer.batch_decode(label_ids)\n",
        "\n",
        "    # # save references and predictions to a txt file for debugging\n",
        "    # with open('refs_and_preds.txt', 'w') as f:\n",
        "    #     for ref, pred in zip(label_str, pred_str):\n",
        "    #         f.write(f\"Ref: {ref}\\n\")\n",
        "    #         f.write(f\"Pred: {pred}\\n\\n\")\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fba6e894",
      "metadata": {
        "id": "fba6e894"
      },
      "source": [
        "### If you want to use LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b44949",
      "metadata": {
        "id": "d2b44949"
      },
      "outputs": [],
      "source": [
        "# lora_config = dict(\n",
        "#     r=32, lora_alpha=64,\n",
        "#     lora_dropout=0.05,\n",
        "#     target_modules=[\n",
        "#         \"linear_k\", \"linear_q\", # change if you want\n",
        "#     ],\n",
        "#     bias=\"none\"\n",
        "# )\n",
        "\n",
        "# peft_config = peft.LoraConfig(\n",
        "#     inference_mode=False,\n",
        "#     **lora_config,\n",
        "# )\n",
        "\n",
        "# model = peft.get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efae4f94",
      "metadata": {
        "id": "efae4f94"
      },
      "outputs": [],
      "source": [
        "# model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06",
      "metadata": {
        "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
      },
      "source": [
        "### Define the Training Configuration\n",
        "\n",
        "Set trainer with different collators for train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b60276",
      "metadata": {
        "id": "b7b60276"
      },
      "outputs": [],
      "source": [
        "class TrainerDifCollators(Trainer):\n",
        "\n",
        "    def __init__(self,  val_data_collator=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.val_data_collator = val_data_collator\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset: Optional[Union[str, Dataset]] = None) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the evaluation [`~torch.utils.data.DataLoader`].\n",
        "\n",
        "        Subclass and override this method if you want to inject some custom behavior.\n",
        "\n",
        "        Args:\n",
        "            eval_dataset (`str` or `torch.utils.data.Dataset`, *optional*):\n",
        "                If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\n",
        "        \"\"\"\n",
        "        if eval_dataset is None and self.eval_dataset is None:\n",
        "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
        "\n",
        "        # If we have persistent workers, don't do a fork bomb especially as eval datasets\n",
        "        # don't change during training\n",
        "        dataloader_key = eval_dataset if isinstance(eval_dataset, str) else \"eval\"\n",
        "        if (\n",
        "            hasattr(self, \"_eval_dataloaders\")\n",
        "            and dataloader_key in self._eval_dataloaders\n",
        "            and self.args.dataloader_persistent_workers\n",
        "        ):\n",
        "            return self.accelerator.prepare(self._eval_dataloaders[dataloader_key])\n",
        "\n",
        "        eval_dataset = (\n",
        "            self.eval_dataset[eval_dataset]\n",
        "            if isinstance(eval_dataset, str)\n",
        "            else eval_dataset\n",
        "            if eval_dataset is not None\n",
        "            else self.eval_dataset\n",
        "        )\n",
        "        data_collator = self.val_data_collator if self.val_data_collator else self.data_collator\n",
        "\n",
        "        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
        "            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
        "        else:\n",
        "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"evaluation\")\n",
        "\n",
        "        dataloader_params = {\n",
        "            \"batch_size\": self.args.eval_batch_size,\n",
        "            \"collate_fn\": data_collator,\n",
        "            \"num_workers\": self.args.dataloader_num_workers,\n",
        "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
        "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
        "        }\n",
        "\n",
        "        if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
        "            dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
        "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
        "            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
        "\n",
        "        # accelerator.free_memory() will destroy the references, so\n",
        "        # we need to store the non-prepared version\n",
        "        eval_dataloader = DataLoader(eval_dataset, **dataloader_params)\n",
        "        if self.args.dataloader_persistent_workers:\n",
        "            if hasattr(self, \"_eval_dataloaders\"):\n",
        "                self._eval_dataloaders[dataloader_key] = eval_dataloader\n",
        "            else:\n",
        "                self._eval_dataloaders = {dataloader_key: eval_dataloader}\n",
        "\n",
        "        return self.accelerator.prepare(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "# Experiment name\n",
        "ex_name = \"gigaam-ctc-test\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./finetune/{ex_name}\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=100,\n",
        "    max_steps=2500,\n",
        "    weight_decay=1e-4,\n",
        "    gradient_checkpointing=False,\n",
        "    # fp16=False,\n",
        "    save_only_model=True,\n",
        "    dataloader_num_workers=2,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=4,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=[\"wandb\"],\n",
        "    load_best_model_at_end=True,\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        "    metric_for_best_model=\"val_wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        "    seed=SEED,\n",
        "    run_name=ex_name,\n",
        ")\n",
        "\n",
        "trainer = TrainerDifCollators(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=audio_dataset[\"train\"],\n",
        "    eval_dataset={\"val\": audio_dataset[\"validation\"]},\n",
        "    data_collator=data_collator,\n",
        "    val_data_collator=val_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "# processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f404cf9-4345-468c-8196-4bd101d9bd51",
      "metadata": {
        "id": "7f404cf9-4345-468c-8196-4bd101d9bd51"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d27682",
      "metadata": {
        "id": "a1d27682"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}